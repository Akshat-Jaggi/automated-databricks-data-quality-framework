# AutoDQ - Automated Databricks Data Quality Framework (Databricks + YAML + PySpark)

A **scalable and reusable framework** to automate **data quality checks** in Databricks.  
This project enables data teams to define validation rules in a simple **YAML configuration file** and execute them at scale using **PySpark**. Validation results are logged in **Delta Lake** for auditing and trend analysis, ensuring data reliability and integrity across the pipeline.  


## Project Overview
Data quality is critical for trustworthy analytics. This framework provides:  
- A **YAML-driven rule engine** to validate datasets.  
- Automated execution as a **Databricks Job**.  
- **Centralized logging** of validation results in Delta tables.  
- Scalable PySpark implementation for large datasets.  

The system is designed for **scalability, reusability, and governance**, making it a strong foundation for enterprise-grade data validation.  

## Key Features
- **Configurable Rules** â€“ Define checks for uniqueness, completeness, validity, and row counts directly in a YAML file.  
- **Automated Execution** â€“ Run seamlessly as a scheduled Databricks Job.  
- **Centralized Logging** â€“ Store validation results in a Delta table for historical audits and quality trend monitoring.  
- **Scalable** â€“ PySpark implementation ensures efficient handling of large datasets.  

##  Tech Stack
- **Databricks** â€“ Core execution platform  
- **PySpark** â€“ Scalable data processing  
- **Delta Lake** â€“ Reliable storage & logging of validation results  
- **YAML** â€“ Externalized configuration for flexible rules  

## ðŸ“‚ Repository Structure
