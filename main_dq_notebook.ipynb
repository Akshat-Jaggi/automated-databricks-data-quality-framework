{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd2764c-4ac9-4366-a443-ef8d806d9c7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Reading the YAML Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a819f4-3841-45f5-be8a-5c42f0cb1ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c9096d-9ae0-409b-9940-d43aa1fcfc83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8126f0-ab9c-40f3-96ae-73d2a729e936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_quality_checks': {'workspace.default.source_table_perfect': {'uniqueness': ['customer_id', 'order_id'], 'completeness': ['order_id', 'order_date'], 'validity': [{'column': 'order_date', 'check': \"order_date > '2023-01-01'\"}], 'row_count': {'min': 10, 'max': 100000}}, 'workspace.default.source_table_uniqueness_fail': {'uniqueness': ['customer_id', 'order_id'], 'completeness': ['order_id', 'order_date'], 'validity': [{'column': 'order_date', 'check': \"order_date >= '2024-01-01'\"}], 'row_count': {'min': 5, 'max': 10}}, 'workspace.default.source_table_completeness_validity_fail': {'uniqueness': ['customer_id', 'order_id'], 'completeness': ['order_id', 'order_date'], 'validity': [{'column': 'order_date', 'check': \"order_date >= '2024-01-01'\"}], 'row_count': {'min': 5, 'max': 10}}, 'workspace.default.source_table_row_count_fail': {'uniqueness': ['customer_id', 'order_id'], 'completeness': ['order_id', 'order_date'], 'validity': [{'column': 'order_date', 'check': \"order_date >= '2024-01-01'\"}], 'row_count': {'min': 1000, 'max': 100000}}}}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228a844c-2d9e-4500-afa2-ad3be5383843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_items([('workspace.default.source_table_perfect', {'uniqueness': ['customer_id', 'order_id'], 'completeness': ['order_id', 'order_date'], 'validity': [{'column': 'order_date', 'check': \"order_date > '2023-01-01'\"}], 'row_count': {'min': 10, 'max': 100000}}), ('workspace.default.source_table_uniqueness_fail', {'uniqueness': ['customer_id', 'order_id'], 'completeness': ['order_id', 'order_date'], 'validity': [{'column': 'order_date', 'check': \"order_date >= '2024-01-01'\"}], 'row_count': {'min': 5, 'max': 10}}), ('workspace.default.source_table_completeness_validity_fail', {'uniqueness': ['customer_id', 'order_id'], 'completeness': ['order_id', 'order_date'], 'validity': [{'column': 'order_date', 'check': \"order_date >= '2024-01-01'\"}], 'row_count': {'min': 5, 'max': 10}}), ('workspace.default.source_table_row_count_fail', {'uniqueness': ['customer_id', 'order_id'], 'completeness': ['order_id', 'order_date'], 'validity': [{'column': 'order_date', 'check': \"order_date >= '2024-01-01'\"}], 'row_count': {'min': 1000, 'max': 100000}})])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"data_quality_checks\"].items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71ac5ece-2bae-495b-a25c-608d9ceb6eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Sample Tables to check this Data Validation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c3f78f4-bc82-4476-8def-85a4024ba29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The Perfect Table** will have all the validation checks passed and there won't be any failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e80836-a6b7-410b-8092-9f35f44175de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "perfect_data = [\n",
    "    (1, \"ORD001\", \"2024-01-01\", 150.75, \"processed\"),\n",
    "    (2, \"ORD002\", \"2024-01-02\", 200.00, \"shipped\"),\n",
    "    (3, \"ORD003\", \"2024-01-03\", 50.25, \"delivered\"),\n",
    "    (4, \"ORD004\", \"2024-01-04\", 300.50, \"processed\"),\n",
    "    (5, \"ORD005\", \"2024-01-05\", 75.10, \"shipped\"),\n",
    "    (6, \"ORD006\", \"2024-01-06\", 120.00, \"delivered\"),\n",
    "    (7, \"ORD007\", \"2024-01-07\", 180.00, \"processed\"),\n",
    "    (8, \"ORD008\", \"2024-01-08\", 90.00, \"shipped\"),\n",
    "    (9, \"ORD009\", \"2024-01-09\", 60.50, \"delivered\"),\n",
    "    (10, \"ORD010\", \"2024-01-10\", 250.00, \"processed\")\n",
    "]\n",
    "columns = [\"customer_id\", \"order_id\", \"order_date\", \"total_amount\", \"status\"]\n",
    "perfect_df = spark.createDataFrame(perfect_data, schema=columns)\n",
    "\n",
    "# Write this to a table\n",
    "perfect_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"source_table_perfect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e3d7481-36c3-4015-a0dd-2e04a863480f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The Uniqueness Failure Dataset** has a duplicate \"customer_id\" and a duplicate \"order_id\", which should fail the uniqueness check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ea3bad-309b-4792-bdcd-cf9dd4b35502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uniqueness_fail_data = [\n",
    "    (1, \"ORD001\", \"2024-01-01\", 150.75, \"processed\"),\n",
    "    (2, \"ORD002\", \"2024-01-02\", 200.00, \"shipped\"),\n",
    "    (3, \"ORD003\", \"2024-01-03\", 50.25, \"delivered\"),\n",
    "    (3, \"ORD003\", \"2024-01-04\", 300.50, \"processed\"), # Duplicate customer_id and order_id\n",
    "    (5, \"ORD005\", \"2024-01-05\", 75.10, \"shipped\")\n",
    "]\n",
    "columns = [\"customer_id\", \"order_id\", \"order_date\", \"total_amount\", \"status\"]\n",
    "uniqueness_fail_df = spark.createDataFrame(uniqueness_fail_data, schema=columns)\n",
    "\n",
    "# Write this to a table\n",
    "uniqueness_fail_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"source_table_uniqueness_fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d7f9e82-325f-4837-8fb2-d8c9f6a42d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The Completeness and Validity Failure Dataset**\n",
    "includes **NULL** values in key columns and a date that violates your validity rule, which should trigger failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f298dc1f-c0b4-43ac-b4ce-44fb3ac59ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completeness_validity_fail_data = [\n",
    "    (1, \"ORD001\", \"2024-01-01\", 150.75, \"processed\"),\n",
    "    (2, \"ORD002\", None, 200.00, \"shipped\"),  # Null order_date\n",
    "    (3, \"ORD003\", \"2022-12-31\", 50.25, \"delivered\"), # Fails validity check (< '2023-01-01')\n",
    "    (4, None, \"2024-01-04\", 300.50, \"processed\"), # Null order_id\n",
    "    (5, \"ORD005\", \"2024-01-05\", 75.10, \"shipped\")\n",
    "]\n",
    "columns = [\"customer_id\", \"order_id\", \"order_date\", \"total_amount\", \"status\"]\n",
    "completeness_validity_fail_df = spark.createDataFrame(completeness_validity_fail_data, schema=columns)\n",
    "\n",
    "# Write this to a table\n",
    "completeness_validity_fail_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"source_table_completeness_validity_fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eec820f-647f-4be4-ac6e-66ed3a216a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The Row Count Failure Dataset**\n",
    "has a very small number of rows, which should fall below the minimum row count threshold set in our config.yaml and will hence trigger a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3a044c-ce50-4df0-abbe-58d54a4935d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row_count_fail_data = [\n",
    "    (1, \"ORD001\", \"2024-01-01\", 150.75, \"processed\"),\n",
    "    (2, \"ORD002\", \"2024-01-02\", 200.00, \"shipped\")\n",
    "]\n",
    "columns = [\"customer_id\", \"order_id\", \"order_date\", \"total_amount\", \"status\"]\n",
    "row_count_fail_df = spark.createDataFrame(row_count_fail_data, schema=columns)\n",
    "\n",
    "# Write this to a table\n",
    "row_count_fail_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"source_table_row_count_fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4940b7a0-46a2-48f6-b0d1-7dff98ccf881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Quality Checks and Logging Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c50f22-b258-4b35-a1f2-3f23eab345f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, countDistinct, sum, lit\n",
    "\n",
    "def run_data_quality_checks(df, dq_rules):\n",
    "    \"\"\"\n",
    "    Executes data quality checks on a DataFrame based on provided rules.\n",
    "    Returns a dictionary of results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check Uniqueness\n",
    "    if 'uniqueness' in dq_rules:\n",
    "        for column in dq_rules['uniqueness']:\n",
    "            total_count = df.count()\n",
    "            distinct_count = df.select(countDistinct(column)).first()[0]\n",
    "            uniqueness_pct = (distinct_count / total_count) * 100\n",
    "            results[f\"uniqueness_{column}\"] = {\n",
    "                \"status\": \"PASS\" if uniqueness_pct == 100 else \"FAIL\",\n",
    "                \"message\": f\"Uniqueness for '{column}': {uniqueness_pct:.2f}%\"\n",
    "            }\n",
    "    \n",
    "    # Check Completeness\n",
    "    if 'completeness' in dq_rules:\n",
    "        for column in dq_rules['completeness']:\n",
    "            completeness_count = df.filter(col(column).isNotNull()).count()\n",
    "            total_count = df.count()\n",
    "            completeness_pct = (completeness_count / total_count) * 100\n",
    "            results[f\"completeness_{column}\"] = {\n",
    "                \"status\": \"PASS\" if completeness_pct == 100 else \"FAIL\",\n",
    "                \"message\": f\"Completeness for '{column}': {completeness_pct:.2f}%\"\n",
    "            }\n",
    "\n",
    "    # Check Validity (using a generic expression)\n",
    "    if 'validity' in dq_rules:\n",
    "        for rule in dq_rules['validity']:\n",
    "            column = rule['column']\n",
    "            check_expression = rule['check']\n",
    "            valid_count = df.filter(check_expression).count()\n",
    "            total_count = df.count()\n",
    "            validity_pct = (valid_count / total_count) * 100\n",
    "            results[f\"validity_{column}\"] = {\n",
    "                \"status\": \"PASS\" if validity_pct == 100 else \"FAIL\",\n",
    "                \"message\": f\"Validity for '{column}': {validity_pct:.2f}%\"\n",
    "            }\n",
    "\n",
    "    # Check Row Count\n",
    "    if 'row_count' in dq_rules:\n",
    "        row_count = df.count()\n",
    "        min_rows = dq_rules['row_count'].get('min', 0)\n",
    "        max_rows = dq_rules['row_count'].get('max', float('inf'))\n",
    "        status = \"PASS\" if min_rows <= row_count <= max_rows else \"FAIL\"\n",
    "        results[\"row_count_check\"] = {\n",
    "            \"status\": status,\n",
    "            \"message\": f\"Row count: {row_count} (Expected between {min_rows} and {max_rows})\"\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def log_results(results, table_name):\n",
    "    \"\"\"\n",
    "    Logs data quality check results to a Delta table for monitoring and alerting.\n",
    "    \"\"\"\n",
    "    # Convert results dictionary to a list of rows\n",
    "    rows = []\n",
    "    for check_name, result in results.items():\n",
    "        rows.append((table_name, check_name, result['status'], result['message']))\n",
    "\n",
    "    # Define schema for the results table\n",
    "    schema = \"table_name STRING, check_name STRING, status STRING, message STRING\"\n",
    "    \n",
    "    # Create DataFrame and write to a Delta table\n",
    "    results_df = spark.createDataFrame(rows, schema=schema)\n",
    "    results_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"dq_metrics_results\")\n",
    "    print(\"DQ results logged successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b8f4ef7-df1e-43c1-b07a-e34c2d0f02c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Scanning YAML and doing Data Quality Validation for the Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa89dcbe-bc6a-4ffb-9508-b58072cffc24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DQ checks for table: workspace.default.source_table_perfect\nDQ results logged successfully!\nAll DQ checks passed for workspace.default.source_table_perfect.\nRunning DQ checks for table: workspace.default.source_table_uniqueness_fail\nDQ results logged successfully!\nDQ checks failed for workspace.default.source_table_uniqueness_fail. Triggering alert.\nData Quality Alert: workspace.default.source_table_uniqueness_fail - FAILED Data Quality Alert for table 'workspace.default.source_table_uniqueness_fail':\n\n- uniqueness_customer_id: Uniqueness for 'customer_id': 80.00%\n- uniqueness_order_id: Uniqueness for 'order_id': 80.00%\n\nRunning DQ checks for table: workspace.default.source_table_completeness_validity_fail\nDQ results logged successfully!\nDQ checks failed for workspace.default.source_table_completeness_validity_fail. Triggering alert.\nData Quality Alert: workspace.default.source_table_completeness_validity_fail - FAILED Data Quality Alert for table 'workspace.default.source_table_completeness_validity_fail':\n\n- uniqueness_order_id: Uniqueness for 'order_id': 80.00%\n- completeness_order_id: Completeness for 'order_id': 80.00%\n- completeness_order_date: Completeness for 'order_date': 80.00%\n- validity_order_date: Validity for 'order_date': 60.00%\n\nRunning DQ checks for table: workspace.default.source_table_row_count_fail\nDQ results logged successfully!\nDQ checks failed for workspace.default.source_table_row_count_fail. Triggering alert.\nData Quality Alert: workspace.default.source_table_row_count_fail - FAILED Data Quality Alert for table 'workspace.default.source_table_row_count_fail':\n\n- row_count_check: Row count: 2 (Expected between 1000 and 100000)\n\n"
     ]
    }
   ],
   "source": [
    "# Loop Through Each Table and Run Checks \n",
    "for table_name, dq_rules in config['data_quality_checks'].items():\n",
    "    print(f\"Running DQ checks for table: {table_name}\")\n",
    "\n",
    "    # Read Data \n",
    "    try:\n",
    "        df = spark.table(table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading table {table_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    #  Step 4: Run Checks and Log Results \n",
    "    results = run_data_quality_checks(df, dq_rules)\n",
    "    log_results(results, table_name)\n",
    "\n",
    "    #  Step 5: Check for Failures and Trigger Alerting \n",
    "    failures = {k: v for k, v in results.items() if v['status'] == 'FAIL'}\n",
    "    if failures:\n",
    "        print(f\"DQ checks failed for {table_name}. Triggering alert.\")\n",
    "        \n",
    "        # Build the email body\n",
    "        email_body = f\"Data Quality Alert for table '{table_name}':\\n\\n\"\n",
    "        for check_name, failure in failures.items():\n",
    "            email_body += f\"- {check_name}: {failure['message']}\\n\"\n",
    "        \n",
    "        # Call the alerting function\n",
    "        print(f\"Data Quality Alert: {table_name} - FAILED\", email_body)\n",
    "    else:\n",
    "        print(f\"All DQ checks passed for {table_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8252ba90-1e5a-4cf4-ae0f-f9b334af0258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking Generated Logs for Better Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51e86b4-a773-4932-a242-5d0e0083107c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_name</th><th>check_name</th><th>status</th><th>message</th></tr></thead><tbody><tr><td>workspace.default.source_table_completeness_validity_fail</td><td>uniqueness_customer_id</td><td>PASS</td><td>Uniqueness for 'customer_id': 100.00%</td></tr><tr><td>workspace.default.source_table_completeness_validity_fail</td><td>uniqueness_order_id</td><td>FAIL</td><td>Uniqueness for 'order_id': 80.00%</td></tr><tr><td>workspace.default.source_table_completeness_validity_fail</td><td>completeness_order_id</td><td>FAIL</td><td>Completeness for 'order_id': 80.00%</td></tr><tr><td>workspace.default.source_table_completeness_validity_fail</td><td>completeness_order_date</td><td>FAIL</td><td>Completeness for 'order_date': 80.00%</td></tr><tr><td>workspace.default.source_table_completeness_validity_fail</td><td>validity_order_date</td><td>FAIL</td><td>Validity for 'order_date': 60.00%</td></tr><tr><td>workspace.default.source_table_completeness_validity_fail</td><td>row_count_check</td><td>PASS</td><td>Row count: 5 (Expected between 5 and 10)</td></tr><tr><td>workspace.default.source_table_uniqueness_fail</td><td>uniqueness_customer_id</td><td>FAIL</td><td>Uniqueness for 'customer_id': 80.00%</td></tr><tr><td>workspace.default.source_table_uniqueness_fail</td><td>uniqueness_order_id</td><td>FAIL</td><td>Uniqueness for 'order_id': 80.00%</td></tr><tr><td>workspace.default.source_table_uniqueness_fail</td><td>completeness_order_id</td><td>PASS</td><td>Completeness for 'order_id': 100.00%</td></tr><tr><td>workspace.default.source_table_uniqueness_fail</td><td>completeness_order_date</td><td>PASS</td><td>Completeness for 'order_date': 100.00%</td></tr><tr><td>workspace.default.source_table_uniqueness_fail</td><td>validity_order_date</td><td>PASS</td><td>Validity for 'order_date': 100.00%</td></tr><tr><td>workspace.default.source_table_uniqueness_fail</td><td>row_count_check</td><td>PASS</td><td>Row count: 5 (Expected between 5 and 10)</td></tr><tr><td>workspace.default.source_table_row_count_fail</td><td>uniqueness_customer_id</td><td>PASS</td><td>Uniqueness for 'customer_id': 100.00%</td></tr><tr><td>workspace.default.source_table_row_count_fail</td><td>uniqueness_order_id</td><td>PASS</td><td>Uniqueness for 'order_id': 100.00%</td></tr><tr><td>workspace.default.source_table_row_count_fail</td><td>completeness_order_id</td><td>PASS</td><td>Completeness for 'order_id': 100.00%</td></tr><tr><td>workspace.default.source_table_row_count_fail</td><td>completeness_order_date</td><td>PASS</td><td>Completeness for 'order_date': 100.00%</td></tr><tr><td>workspace.default.source_table_row_count_fail</td><td>validity_order_date</td><td>PASS</td><td>Validity for 'order_date': 100.00%</td></tr><tr><td>workspace.default.source_table_row_count_fail</td><td>row_count_check</td><td>FAIL</td><td>Row count: 2 (Expected between 1000 and 100000)</td></tr><tr><td>workspace.default.source_table_perfect</td><td>uniqueness_customer_id</td><td>PASS</td><td>Uniqueness for 'customer_id': 100.00%</td></tr><tr><td>workspace.default.source_table_perfect</td><td>uniqueness_order_id</td><td>PASS</td><td>Uniqueness for 'order_id': 100.00%</td></tr><tr><td>workspace.default.source_table_perfect</td><td>completeness_order_id</td><td>PASS</td><td>Completeness for 'order_id': 100.00%</td></tr><tr><td>workspace.default.source_table_perfect</td><td>completeness_order_date</td><td>PASS</td><td>Completeness for 'order_date': 100.00%</td></tr><tr><td>workspace.default.source_table_perfect</td><td>validity_order_date</td><td>PASS</td><td>Validity for 'order_date': 100.00%</td></tr><tr><td>workspace.default.source_table_perfect</td><td>row_count_check</td><td>PASS</td><td>Row count: 10 (Expected between 10 and 100000)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "workspace.default.source_table_completeness_validity_fail",
         "uniqueness_customer_id",
         "PASS",
         "Uniqueness for 'customer_id': 100.00%"
        ],
        [
         "workspace.default.source_table_completeness_validity_fail",
         "uniqueness_order_id",
         "FAIL",
         "Uniqueness for 'order_id': 80.00%"
        ],
        [
         "workspace.default.source_table_completeness_validity_fail",
         "completeness_order_id",
         "FAIL",
         "Completeness for 'order_id': 80.00%"
        ],
        [
         "workspace.default.source_table_completeness_validity_fail",
         "completeness_order_date",
         "FAIL",
         "Completeness for 'order_date': 80.00%"
        ],
        [
         "workspace.default.source_table_completeness_validity_fail",
         "validity_order_date",
         "FAIL",
         "Validity for 'order_date': 60.00%"
        ],
        [
         "workspace.default.source_table_completeness_validity_fail",
         "row_count_check",
         "PASS",
         "Row count: 5 (Expected between 5 and 10)"
        ],
        [
         "workspace.default.source_table_uniqueness_fail",
         "uniqueness_customer_id",
         "FAIL",
         "Uniqueness for 'customer_id': 80.00%"
        ],
        [
         "workspace.default.source_table_uniqueness_fail",
         "uniqueness_order_id",
         "FAIL",
         "Uniqueness for 'order_id': 80.00%"
        ],
        [
         "workspace.default.source_table_uniqueness_fail",
         "completeness_order_id",
         "PASS",
         "Completeness for 'order_id': 100.00%"
        ],
        [
         "workspace.default.source_table_uniqueness_fail",
         "completeness_order_date",
         "PASS",
         "Completeness for 'order_date': 100.00%"
        ],
        [
         "workspace.default.source_table_uniqueness_fail",
         "validity_order_date",
         "PASS",
         "Validity for 'order_date': 100.00%"
        ],
        [
         "workspace.default.source_table_uniqueness_fail",
         "row_count_check",
         "PASS",
         "Row count: 5 (Expected between 5 and 10)"
        ],
        [
         "workspace.default.source_table_row_count_fail",
         "uniqueness_customer_id",
         "PASS",
         "Uniqueness for 'customer_id': 100.00%"
        ],
        [
         "workspace.default.source_table_row_count_fail",
         "uniqueness_order_id",
         "PASS",
         "Uniqueness for 'order_id': 100.00%"
        ],
        [
         "workspace.default.source_table_row_count_fail",
         "completeness_order_id",
         "PASS",
         "Completeness for 'order_id': 100.00%"
        ],
        [
         "workspace.default.source_table_row_count_fail",
         "completeness_order_date",
         "PASS",
         "Completeness for 'order_date': 100.00%"
        ],
        [
         "workspace.default.source_table_row_count_fail",
         "validity_order_date",
         "PASS",
         "Validity for 'order_date': 100.00%"
        ],
        [
         "workspace.default.source_table_row_count_fail",
         "row_count_check",
         "FAIL",
         "Row count: 2 (Expected between 1000 and 100000)"
        ],
        [
         "workspace.default.source_table_perfect",
         "uniqueness_customer_id",
         "PASS",
         "Uniqueness for 'customer_id': 100.00%"
        ],
        [
         "workspace.default.source_table_perfect",
         "uniqueness_order_id",
         "PASS",
         "Uniqueness for 'order_id': 100.00%"
        ],
        [
         "workspace.default.source_table_perfect",
         "completeness_order_id",
         "PASS",
         "Completeness for 'order_id': 100.00%"
        ],
        [
         "workspace.default.source_table_perfect",
         "completeness_order_date",
         "PASS",
         "Completeness for 'order_date': 100.00%"
        ],
        [
         "workspace.default.source_table_perfect",
         "validity_order_date",
         "PASS",
         "Validity for 'order_date': 100.00%"
        ],
        [
         "workspace.default.source_table_perfect",
         "row_count_check",
         "PASS",
         "Row count: 10 (Expected between 10 and 100000)"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "check_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "message",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.table(\"workspace.default.dq_metrics_results\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66aa5c12-eded-4b39-808d-0bf27dbbce8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note -** As expected we are having 6 tests in total based on our YAML file parameters and we did it for 4 tables in total, so we have 24 records in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "635633d5-9123-4690-b0b2-e547eb5c47e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7095237351361517,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "main_dq_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}